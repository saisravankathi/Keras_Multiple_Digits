{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will create the data to be trained along with the labels.\n",
    "The problem statement for this can be found in the deeplizard youtube channel #3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-28-97ba68f4ae73>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-28-97ba68f4ae73>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    train_samples.append(randint(13,64))\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "for i in range(50):\n",
    "    train_samples.append(randint(13,64))\n",
    "    train_labels.append(0)\n",
    "    \n",
    "    train_samples.append(randint(65,100))\n",
    "    train_labels.append(1)\n",
    "    \n",
    "for i in range(1000):\n",
    "    train_samples.append(randint(13,64))\n",
    "    train_labels.append(1)\n",
    "    \n",
    "    train_samples.append(randint(65,100))\n",
    "    train_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 94, 35, 65, 46, 83, 18, 82, 56, 93, 63, 72, 28, 95, 19, 76, 40, 88, 35, 65, 56, 99, 20, 69, 51, 99, 20, 78, 33, 84, 41, 87, 64, 93, 48, 97, 41, 75, 36, 73, 63, 89, 14, 92, 31, 81, 26, 87, 59, 73, 13, 85, 21, 99, 41, 91, 50, 74, 61, 66, 26, 88, 48, 69, 25, 71, 45, 91, 41, 74, 32, 84, 61, 68, 22, 85, 58, 98, 25, 86, 61, 82, 28, 95, 63, 67, 55, 66, 39, 92, 40, 88, 64, 65, 26, 84, 50, 91, 13, 84, 33, 81, 36, 68, 15, 70, 30, 91, 18, 70, 32, 77, 25, 94, 27, 88, 53, 85, 57, 98, 26, 67, 17, 75, 13, 95, 46, 68, 63, 66, 53, 67, 56, 75, 25, 92, 36, 92, 56, 84, 33, 73, 31, 83, 60, 91, 20, 73, 25, 97, 58, 67, 19, 73, 22, 71, 56, 88, 42, 85, 29, 71, 51, 91, 22, 76, 38, 68, 29, 69, 59, 88, 43, 90, 36, 96, 43, 74, 38, 96, 20, 70, 47, 99, 13, 72, 21, 71, 44, 86, 35, 88, 14, 97, 44, 92, 26, 78, 33, 83, 51, 73, 26, 92, 42, 74, 14, 88, 19, 66, 64, 85, 51, 69, 28, 85, 25, 97, 33, 99, 57, 90, 19, 94, 60, 81, 25, 81, 48, 66, 45, 95, 14, 98, 29, 72, 20, 77, 45, 79, 34, 74, 37, 83, 63, 94, 30, 76, 39, 73, 41, 67, 61, 74, 57, 70, 30, 73, 55, 97, 51, 98, 24, 89, 35, 99, 40, 65, 50, 79, 51, 88, 42, 68, 64, 96, 21, 94, 62, 90, 43, 67, 56, 85, 16, 71, 59, 100, 59, 96, 51, 84, 54, 75, 24, 95, 44, 100, 56, 70, 57, 94, 47, 68, 37, 91, 59, 73, 47, 98, 46, 100, 15, 100, 56, 92, 53, 97, 24, 94, 35, 78, 14, 75, 23, 69, 13, 65, 27, 85, 58, 100, 43, 74, 64, 67, 50, 98, 36, 73, 38, 96, 45, 94, 33, 85, 63, 100, 55, 69, 37, 94, 53, 79, 14, 92, 24, 99, 59, 97, 48, 83, 53, 73, 38, 83, 52, 66, 18, 73, 52, 66, 34, 97, 36, 95, 17, 88, 43, 68, 15, 81, 48, 65, 49, 66, 42, 85, 35, 86, 40, 92, 39, 86, 63, 66, 53, 70, 30, 79, 62, 90, 37, 76, 61, 67, 55, 72, 44, 100, 28, 100, 44, 90, 34, 69, 56, 92, 32, 77, 42, 74, 54, 69, 49, 78, 28, 74, 64, 78, 63, 96, 19, 71, 55, 80, 44, 82, 44, 72, 51, 80, 39, 83, 42, 100, 37, 93, 58, 91, 16, 69, 14, 95, 33, 78, 61, 71, 62, 87, 38, 90, 29, 82, 42, 76, 59, 70, 47, 65, 19, 93, 50, 67, 23, 78, 13, 96, 45, 84, 27, 98, 44, 98, 32, 97, 60, 70, 43, 83, 57, 91, 56, 89, 41, 66, 47, 71, 64, 87, 64, 90, 46, 89, 16, 71, 20, 87, 21, 91, 54, 92, 51, 99, 15, 86, 45, 86, 15, 95, 16, 77, 59, 81, 51, 77, 62, 100, 22, 78, 55, 82, 33, 89, 16, 92, 18, 81, 51, 95, 35, 80, 57, 91, 51, 86, 64, 95, 22, 72, 30, 87, 33, 83, 29, 88, 20, 73, 23, 66, 16, 90, 51, 90, 40, 82, 50, 80, 34, 92, 42, 97, 42, 93, 36, 84, 50, 90, 14, 70, 42, 77, 27, 80, 53, 73, 24, 82, 17, 84, 19, 75, 51, 76, 30, 94, 55, 83, 60, 71, 60, 94, 13, 96, 30, 74, 52, 67, 63, 77, 21, 97, 25, 76, 35, 79, 62, 86, 42, 80, 25, 88, 41, 83, 38, 94, 47, 92, 16, 88, 14, 76, 23, 99, 35, 92, 59, 77, 20, 84, 50, 88, 32, 99, 55, 83, 61, 92, 55, 90, 37, 74, 43, 90, 16, 94, 14, 92, 15, 94, 22, 91, 37, 86, 26, 90, 42, 98, 51, 97, 55, 81, 57, 82, 24, 87, 53, 73, 14, 83, 32, 82, 18, 89, 14, 85, 16, 93, 47, 96, 42, 73, 43, 94, 24, 75, 28, 84, 39, 65, 21, 86, 63, 71, 25, 96, 49, 76, 38, 77, 46, 84, 14, 94, 47, 92, 41, 94, 36, 87, 53, 100, 60, 99, 42, 66, 64, 73, 40, 86, 63, 90, 26, 76, 39, 71, 61, 69, 61, 91, 64, 67, 32, 86, 58, 81, 13, 86, 63, 79, 23, 100, 53, 65, 18, 77, 64, 98, 59, 69, 24, 70, 15, 66, 20, 66, 56, 85, 48, 68, 52, 100, 49, 84, 41, 81, 13, 84, 25, 73, 27, 70, 20, 87, 63, 84, 64, 80, 35, 69, 50, 72, 62, 80, 28, 67, 16, 69, 37, 95, 54, 80, 53, 79, 15, 67, 37, 91, 42, 89, 28, 81, 29, 98, 48, 93, 54, 67, 28, 99, 22, 80, 16, 100, 51, 78, 22, 81, 23, 95, 24, 90, 37, 91, 40, 78, 37, 87, 41, 93, 42, 69, 60, 86, 64, 83, 26, 69, 39, 79, 28, 75, 53, 83, 52, 75, 51, 85, 40, 76, 25, 82, 63, 74, 52, 84, 43, 85, 57, 70, 28, 80, 22, 80, 14, 82, 35, 81, 33, 88, 64, 69, 15, 98, 38, 80, 18, 89, 53, 89, 49, 81, 34, 87, 51, 100, 63, 83, 51, 97, 28, 70, 54, 98, 26, 90, 16, 74, 58, 98, 64, 93, 51, 96, 30, 75, 57, 89, 13, 99, 59, 72, 18, 97, 29, 92, 40, 71, 64, 88, 27, 75, 48, 88, 27, 65, 52, 79, 43, 82, 59, 74, 23, 99, 35, 94, 50, 72, 63, 66, 62, 85, 17, 80, 56, 77, 30, 90, 32, 81, 16, 69, 30, 70, 19, 67, 15, 66, 54, 70, 58, 70, 26, 74, 40, 73, 35, 100, 49, 98, 45, 88, 29, 95, 36, 79, 43, 70, 36, 93, 23, 85, 59, 74, 22, 77, 26, 77, 31, 100, 54, 87, 53, 79, 20, 73, 52, 91, 64, 100, 36, 95, 58, 76, 41, 87, 40, 90, 13, 71, 42, 93, 47, 85, 29, 82, 30, 82, 56, 68, 52, 78, 41, 100, 32, 100, 33, 65, 26, 79, 43, 87, 53, 91, 42, 71, 23, 93, 63, 69, 49, 81, 61, 91, 37, 73, 17, 79, 24, 74, 27, 93, 51, 94, 38, 84, 47, 92, 29, 76, 30, 78, 31, 70, 58, 68, 16, 96, 13, 86, 14, 82, 49, 90, 18, 79, 36, 91, 24, 100, 28, 68, 55, 76, 56, 68, 49, 95, 44, 100, 16, 70, 29, 81, 19, 66, 44, 85, 64, 75, 42, 97, 16, 77, 16, 83, 62, 87, 56, 97, 58, 88, 43, 66, 57, 71, 40, 91, 27, 66, 31, 75, 61, 80, 14, 87, 43, 85, 64, 94, 51, 65, 21, 91, 60, 92, 28, 69, 24, 68, 54, 68, 60, 67, 31, 67, 46, 82, 24, 93, 28, 97, 35, 70, 60, 93, 16, 78, 14, 83, 48, 69, 58, 99, 35, 89, 28, 77, 13, 85, 32, 83, 50, 88, 58, 85, 63, 94, 56, 80, 17, 92, 16, 86, 41, 99, 32, 95, 63, 100, 51, 84, 41, 65, 50, 93, 45, 65, 62, 99, 62, 67, 55, 80, 53, 74, 55, 67, 63, 68, 38, 98, 60, 75, 51, 89, 48, 67, 38, 75, 46, 95, 48, 72, 61, 81, 53, 90, 34, 68, 16, 96, 52, 70, 13, 65, 26, 90, 37, 70, 58, 68, 39, 94, 36, 70, 47, 94, 39, 85, 42, 75, 51, 84, 59, 78, 42, 79, 30, 81, 34, 84, 62, 90, 61, 92, 39, 89, 56, 70, 13, 80, 57, 69, 25, 78, 28, 70, 29, 65, 34, 97, 29, 87, 34, 75, 18, 85, 42, 86, 23, 67, 34, 83, 47, 79, 33, 79, 15, 90, 41, 94, 27, 93, 44, 97, 47, 90, 56, 99, 25, 94, 37, 65, 57, 66, 15, 68, 22, 83, 58, 98, 31, 70, 36, 91, 54, 99, 44, 94, 57, 99, 61, 93, 40, 88, 33, 76, 60, 70, 40, 80, 62, 95, 34, 79, 31, 99, 22, 82, 25, 68, 31, 84, 35, 70, 54, 92, 21, 76, 18, 95, 61, 79, 31, 72, 48, 95, 32, 67, 22, 70, 57, 68, 60, 75, 41, 95, 48, 80, 51, 67, 37, 68, 43, 92, 28, 100, 50, 65, 56, 69, 63, 72, 41, 86, 59, 86, 30, 78, 57, 80, 62, 86, 13, 72, 56, 87, 54, 67, 24, 98, 61, 78, 20, 100, 64, 77, 13, 85, 46, 69, 38, 76, 22, 65, 16, 89, 30, 99, 61, 94, 47, 70, 56, 74, 26, 100, 55, 98, 55, 92, 59, 85, 24, 70, 50, 66, 22, 72, 45, 77, 58, 74, 36, 67, 44, 97, 35, 86, 41, 89, 61, 86, 44, 67, 16, 88, 60, 91, 16, 91, 15, 67, 52, 94, 19, 77, 18, 95, 54, 90, 46, 66, 57, 68, 47, 93, 18, 66, 32, 71, 62, 100, 30, 79, 55, 75, 23, 74, 59, 75, 52, 98, 40, 74, 23, 87, 30, 100, 50, 89, 54, 86, 46, 74, 34, 99, 54, 81, 29, 100, 44, 85, 13, 75, 64, 69, 50, 75, 51, 83, 34, 68, 20, 94, 36, 81, 31, 66, 61, 69, 53, 76, 22, 84, 31, 96, 31, 98, 35, 81, 31, 67, 21, 100, 19, 78, 62, 69, 18, 70, 43, 99, 35, 80, 55, 82, 58, 98, 52, 85, 29, 100, 50, 66, 46, 84, 61, 71, 40, 65, 13, 71, 26, 84, 42, 77, 27, 84, 52, 85, 39, 84, 30, 86, 61, 86, 42, 72, 26, 75, 18, 72, 28, 83, 46, 72, 13, 78, 41, 89, 45, 67, 33, 77, 47, 69, 15, 96, 34, 70, 56, 100, 14, 78, 51, 88, 24, 68, 45, 83, 44, 83, 61, 72, 55, 94, 17, 82, 30, 89, 58, 98, 36, 83, 61, 83, 48, 68, 44, 84, 64, 85, 63, 77, 51, 89, 63, 81, 18, 99, 63, 68, 24, 70, 41, 74, 15, 74, 58, 87, 43, 77, 30, 70, 13, 93, 27, 93, 53, 83, 45, 79, 44, 68, 28, 80, 26, 94, 61, 78, 24, 71, 53, 76, 59, 79, 47, 69, 36, 71, 50, 75, 16, 81, 16, 71, 20, 90, 29, 88, 32, 94, 56, 100, 60, 90, 15, 85, 42, 71, 60, 100, 61, 96, 49, 65, 48, 94, 61, 85, 29, 97, 39, 83, 39, 92, 35, 80, 38, 75, 33, 79, 23, 100, 53, 98, 13, 76, 59, 67, 54, 96, 45, 65, 22, 87, 22, 87, 61, 78, 21, 80, 23, 76, 57, 81, 20, 73, 55, 75, 43, 66, 55, 76, 63, 80, 36, 96, 19, 73, 54, 93, 58, 80, 46, 86, 61, 87, 54, 68, 50, 69, 31, 72, 33, 74, 30, 73, 33, 74, 34, 71, 21, 66, 30, 86, 29, 90, 44, 69, 14, 78, 15, 87, 48, 85, 62, 97, 48, 93, 47, 67, 59, 66, 62, 70, 32, 68, 50, 73, 33, 92, 30, 100, 26, 68, 30, 71, 13, 71, 44, 93, 52, 75, 45, 82, 63, 70, 56, 72, 59, 74, 37, 67, 52, 67, 43, 91, 30, 87, 62, 70, 16, 100, 50, 94, 19, 76, 37, 98, 40, 98, 45, 96, 64, 79, 15, 98, 55, 90, 54, 81, 33, 79, 15, 66, 55, 79, 58, 78, 16, 75, 52, 85, 29, 68, 17, 79, 57, 71, 19, 86, 56, 92, 26, 76, 40, 66, 51, 90, 44, 100, 56, 82, 49, 76, 30, 68, 24, 89, 45, 87, 13, 80, 52, 96, 32, 96, 62, 92, 45, 65, 24, 93, 16, 72, 49, 94, 15, 75, 64, 80, 34, 75, 39, 77, 46, 77, 49, 72, 35, 77, 45, 83, 18, 91, 45, 65, 22, 72, 20, 97, 28, 66, 35, 93, 37, 92, 52, 90, 49, 97, 33, 99, 44, 90, 22, 92, 33, 87, 24, 100, 23, 86, 54, 85, 32, 70, 55, 72, 53, 96, 34, 91, 35, 70, 28, 92, 38, 76, 50, 100, 53, 70, 60, 91, 44, 97, 19, 86, 63, 92, 27, 81, 36, 76, 27, 81, 46, 100, 22, 96, 24, 71, 13, 68, 22, 75, 13, 83, 64, 91, 16, 75, 19, 96, 42, 97, 56, 97, 42, 93, 18, 81, 62, 85, 16, 96, 42, 74, 28, 83, 42, 83, 21, 75, 59, 91, 20, 95, 51, 72, 21, 82, 38, 100, 18, 78, 31, 74, 19, 100, 33, 70, 41, 94, 53, 91, 46, 100, 37, 67, 43, 100, 25, 80, 47, 70]\n"
     ]
    }
   ],
   "source": [
    "print(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lables = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S795641\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_samples = scaler.fit_transform((train_samples).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will train this data to the Sequential model that would be created using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = Sequential([\n",
    "    Dense(16, input_shape=(1,), activation = 'relu'),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dense(2 , activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1890 samples, validate on 210 samples\n",
      "Epoch 1/30\n",
      " - 0s - loss: 0.0711 - acc: 0.9952 - val_loss: 2.7058 - val_acc: 0.5238\n",
      "Epoch 2/30\n",
      " - 0s - loss: 0.0695 - acc: 0.9910 - val_loss: 2.7392 - val_acc: 0.5238\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.0684 - acc: 0.9899 - val_loss: 2.7688 - val_acc: 0.5238\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.0674 - acc: 0.9873 - val_loss: 2.7995 - val_acc: 0.5238\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.0663 - acc: 0.9915 - val_loss: 2.8327 - val_acc: 0.5238\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.0655 - acc: 0.9873 - val_loss: 2.8644 - val_acc: 0.5238\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.0644 - acc: 0.9894 - val_loss: 2.8964 - val_acc: 0.5238\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.0633 - acc: 0.9921 - val_loss: 2.9289 - val_acc: 0.5238\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.0626 - acc: 0.9915 - val_loss: 2.9625 - val_acc: 0.5238\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.0615 - acc: 0.9921 - val_loss: 2.9964 - val_acc: 0.5238\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.0606 - acc: 0.9894 - val_loss: 3.0293 - val_acc: 0.5238\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.0599 - acc: 0.9942 - val_loss: 3.0628 - val_acc: 0.5238\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.0589 - acc: 0.9915 - val_loss: 3.0932 - val_acc: 0.5238\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.0582 - acc: 0.9942 - val_loss: 3.1271 - val_acc: 0.5238\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.0573 - acc: 0.9931 - val_loss: 3.1578 - val_acc: 0.5238\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.0565 - acc: 0.9984 - val_loss: 3.1928 - val_acc: 0.5238\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.0560 - acc: 0.9905 - val_loss: 3.2246 - val_acc: 0.5238\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.0552 - acc: 0.9947 - val_loss: 3.2530 - val_acc: 0.5238\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.0544 - acc: 0.9963 - val_loss: 3.2873 - val_acc: 0.5238\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.0537 - acc: 0.9968 - val_loss: 3.3191 - val_acc: 0.5238\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.0530 - acc: 0.9926 - val_loss: 3.3487 - val_acc: 0.5238\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.0522 - acc: 0.9947 - val_loss: 3.3825 - val_acc: 0.5238\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.0518 - acc: 0.9979 - val_loss: 3.4146 - val_acc: 0.5238\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.0510 - acc: 0.9963 - val_loss: 3.4455 - val_acc: 0.5238\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.0503 - acc: 0.9968 - val_loss: 3.4762 - val_acc: 0.5238\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.0497 - acc: 0.9947 - val_loss: 3.5051 - val_acc: 0.5238\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.0494 - acc: 0.9974 - val_loss: 3.5367 - val_acc: 0.5238\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.0486 - acc: 0.9979 - val_loss: 3.5692 - val_acc: 0.5238\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.0481 - acc: 0.9989 - val_loss: 3.6001 - val_acc: 0.5238\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.0474 - acc: 0.9984 - val_loss: 3.6313 - val_acc: 0.5238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ae28c28eb8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_train_samples, train_labels,validation_split=0.1, batch_size=10, epochs=30, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "#plot_model(model, to_file='model.png')\n",
    "#Plot model is not working because of unable to install graphViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
